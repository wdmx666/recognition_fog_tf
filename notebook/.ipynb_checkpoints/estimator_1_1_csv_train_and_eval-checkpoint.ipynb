{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标:\n",
    "1. 使用CSV文件作为数据的来源，采用train_and_eval(keras自带在train当中)进行训练，\n",
    "2. 进而使用数据验证模型，测试数据测试模型，然后运行一下预测；最后导出模型供使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import math\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'reg-model-02'\n",
    "\n",
    "TRAIN_DATA_FILES_PATTERN = 'data/train-*.csv'\n",
    "VALID_DATA_FILES_PATTERN = 'data/valid-*.csv'\n",
    "TEST_DATA_FILES_PATTERN = 'data/test-*.csv'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = True\n",
    "EXTEND_FEATURE_COLUMNS = True\n",
    "MULTI_THREADING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本步骤\n",
    "1. 定义数据集的元数据(常量到处使用)\n",
    "2. 定义读取CSV的输入函数及其解析(ETL)\n",
    "3. 定义特征列\n",
    "4. 定义估计器的创建函数\n",
    "5. 运行试验，包括运行方案的定义\n",
    "6. 运行模型的评估\n",
    "7. 执行预测和部署已存的模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义数据集元数据\n",
    "- CSV的头和默认值\n",
    "- 数据和类别特征名称\n",
    "- 目标列名称\n",
    "- 无用列名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['key','x','y','alpha','beta','target']\n",
    "HEADER_DEFAULTS = [[0], [0.0], [0.0], ['NA'], ['NA'], [0.0]]\n",
    "NUMERIC_FEATURE_NAMES = ['x', 'y']  \n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'alpha':['ax01', 'ax02'], 'beta':['bx01', 'bx02']}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "TARGET_NAME = 'target'\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义数据数据输入函数\n",
    "- 输入文件名称模式\n",
    "- 使用dataset读入数据\n",
    "- 解析特征\n",
    "- 使用处理\n",
    "- 返回(特征，目标)张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature read from CSV: ['x', 'y', 'alpha', 'beta']\n",
      "Target read from CSV: Tensor(\"IteratorGetNext_5:4\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def process_features(features):\n",
    "    \"\"\"输入数据时张量，采用tf-api，特征名-张量的字典\"\"\"\n",
    "    features['x_2']=tf.square(features['x'])\n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y']) # features['x'] * features['y']\n",
    "    features['dist_xy'] =  tf.sqrt(tf.squared_difference(features['x'],features['y']))\n",
    "\n",
    "def parse_csv_row(csv_row):\n",
    "    \"\"\"返回特征字典-目标元组，供dataset\"\"\"\n",
    "    columns=tf.decode_csv(csv_row,record_defaults=HEADER_DEFAULTS) #按顺序解析\n",
    "    features=dict(zip(HEADER,columns))\n",
    "    for col in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(col)\n",
    "    target= features.pop(TARGET_NAME)\n",
    "    return features,target\n",
    "    \n",
    "    \n",
    "def csv_input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0,num_epochs=None,batch_size=200):\n",
    "    \n",
    "    shuffle=True if mode==tf.estimator.ModeKeys.TRAIN else False\n",
    "    input_file_names=tf.matching_files(files_name_pattern)\n",
    "    dataset = tf.data.TextLineDataset(input_file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    dataset = dataset.map(parse_csv_row)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features,target =iterator.get_next()\n",
    "    return features,target\n",
    "\n",
    "features, target = csv_input_fn(files_name_pattern=\"\")\n",
    "print(\"Feature read from CSV: {}\".format(list(features.keys())))\n",
    "print(\"Target read from CSV: {}\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=tf.data.Dataset.from_tensor_slices([[1,2],[4,9],[4,8]])\n",
    "it= ds.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.定义特征列\n",
    "假设数值列规范化了或者同尺度，否则使用特征列构造器，\n",
    "传递normlizer_fn及其normlisation params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'x': _NumericColumn(key='x', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y': _NumericColumn(key='y', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'x_2': _NumericColumn(key='x_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y_2': _NumericColumn(key='y_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'xy': _NumericColumn(key='xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'dist_xy': _NumericColumn(key='dist_xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'alpha': _VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'beta': _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'alpha_X_beta': _CrossedColumn(keys=(_VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=4, hash_key=None)}\n"
     ]
    }
   ],
   "source": [
    "def extend_feature_columns(feature_columns):\n",
    "    \"\"\"添加交叉列\"\"\"\n",
    "    feature_columns['alpha_X_beta'] = tf.feature_column.crossed_column(\n",
    "        [feature_columns['alpha'], feature_columns['beta']], 4)\n",
    "    return feature_columns\n",
    "\n",
    "def get_feature_columns():\n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = ['x_2', 'y_2', 'xy', 'dist_xy']\n",
    "    all_numeric_feature_names = NUMERIC_FEATURE_NAMES.copy() \n",
    "    if PROCESS_FEATURES: # 进一步选择输入的列\n",
    "        all_numeric_feature_names += CONSTRUCTED_NUMERIC_FEATURES_NAMES\n",
    "    numeric_columns = {feature_name: tf.feature_column.numeric_column(feature_name)\n",
    "                       for feature_name in all_numeric_feature_names}\n",
    "    categorical_column_with_vocabulary = \\\n",
    "        {item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "    # 将准备好的列全出整起来\n",
    "    feature_columns = {}\n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "    # 函数和常量一样是个模块全局对象没有必要传递，直接引用就好\n",
    "    if EXTEND_FEATURE_COLUMNS:\n",
    "        feature_columns = extend_feature_columns(feature_columns)\n",
    "    return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义估计器创建函数\n",
    "- 从特征列中获取数值特征列\n",
    "- 将类型特征列转化成\n",
    "- 使用dense + indicator feature columns + params创建估计器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(run_config,hparams):\n",
    "    feature_columns = list(get_feature_columns())\n",
    "    dense_columns=list(filter(lambda col:isinstance(col,feature_column._NumericColumn),feature_columns))\n",
    "    categorial_columns=list(filter(lambda col:isinstance(col,(feature_column._VocabularyListCategoricalColumn,feature_column._BucketizedColumn)),\n",
    "                                  feature_columns))\n",
    "    indicator_columns = list(map(lambda column: tf.feature_column.indicator_column(column),categorical_columns))\n",
    "    \n",
    "    estimator=tf.estimator.DNNRegressor(feature_columns=dense_columns+indicator_columns,\n",
    "                                       hidden_units=hparams.hidden_units,optimizer=tf.train.AdadeltaOptimizer(),\n",
    "                                       activation_fn=tf.nn.elu,dropout= hparams.dropout_prob,config= run_config)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 运行试验\n",
    "- a. 设置运行参数和模型超参数HParam and RunConfig\n",
    "- b. 定义Serving Function\n",
    "- c. 定义一个Early Stopping Monitor (Hook)\n",
    "- d. 定义TrainSpec and EvaluSpec方案\n",
    "\n",
    "进行了这个多定义，现在模块空间有多少对象？函数对象和实例对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE=12000\n",
    "NUM_EPOCHS=1000\n",
    "BATCH_SIZE=500\n",
    "EVAL_AFTER_SEC=15\n",
    "TOTAL_STEPS=(TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "hparams= tf.contrib.training.HParams(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    hidden_units=[16, 12, 8],\n",
    "    num_buckets = 6,\n",
    "    embedding_size = 3,\n",
    "    max_steps = TOTAL_STEPS,\n",
    "    dropout_prob = 0.001)  #正如spark中paramap\n",
    "\n",
    "run_config=tf.estimator.RunConfig(model_dir='trained_models/{}'.format(MODEL_NAME),tf_random_seed=1983061)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_serving_input():\n",
    "    SERVING_HEADER = ['x','y','alpha','beta']\n",
    "    SERVING_HEADER_DEFAULTS = [[0.0], [0.0], ['NA'], ['NA']]\n",
    "    rows_string_tensor = tf.placeholder(dtype=tf.string,shape=[None], name='csv_rows')\n",
    "    receiver_tensor = {'csv_rows': rows_string_tensor}\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1) #变成2阶，另一axis来放置特征\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=SERVING_HEADER_DEFAULTS)\n",
    "    features = dict(zip(SERVING_HEADER, columns))\n",
    "    if PROCESS_FEATURES: features = process_features(features)\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingHook(tf.train.SessionRunHook):\n",
    "    def __init__(self, early_stopping_rounds=1):\n",
    "        self._best_loss = None\n",
    "        self._early_stopping_rounds = early_stopping_rounds\n",
    "        self._counter = 0\n",
    "        print(\"*** Early Stopping Hook: - Created\")\n",
    "        print(\"*** Early Stopping Hook:: Early Stopping Rounds: {}\".format(self._early_stopping_rounds))\n",
    "    def before_run(self, run_context):  # 调度程序返回给方法的对象\n",
    "        graph = run_context.session.graph\n",
    "        loss_tensor = graph.get_collection(tf.GraphKeys.LOSSES)[1]\n",
    "        return tf.train.SessionRunArgs(loss_tensor)\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        last_loss = run_values.results\n",
    "        print(\"************************\")\n",
    "        print(\"** Evaluation Monitor - Early Stopping **\")\n",
    "        print(\"Early Stopping Hook: Current loss: {}\".format(str(last_loss)))\n",
    "        print(\"Early Stopping Hook: Best loss: {}\".format(str(self._best_loss)))\n",
    "\n",
    "        if self._best_loss is None:\n",
    "            self._best_loss = last_loss\n",
    "        elif last_loss > self._best_loss:\n",
    "            self._counter += 1\n",
    "            print(\"Early Stopping Hook: No improvment! Counter: {}\".format(self._counter))\n",
    "            if self._counter == self._early_stopping_rounds:\n",
    "                run_context.request_stop()\n",
    "                print(\"Early Stopping Hook: Stop Requested: {}\".format(run_context.stop_requested))\n",
    "        else:\n",
    "            self._best_loss = last_loss\n",
    "            self._counter = 0\n",
    "        print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_name_pattern, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0,num_epochs=None,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn=functools.partial(csv_input_fn,\n",
    "                                 files_name_pattern=TRAIN_DATA_FILES_PATTERN,\n",
    "                                 mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                 num_epochs=hparams.num_epochs\n",
    "                                 batch_size=hparams.batch_size)\n",
    "train_spec=tf.estimator.TrainSpec(train_input_fn,max_steps=hparams.max_steps,hooks=None)\n",
    "\n",
    "valid_input_fn =functools.partial(csv_input_fn,\n",
    "                                  files_name_pattern=VALID_DATA_FILES_PATTERN,\n",
    "                                  mode=tf.estimator.ModeKeys.EVAL,\n",
    "                                  num_epochs=hparams.num_epochs,\n",
    "                                  batch_size=hparams.batch_size),\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(valid_input_fn,\n",
    "                                  steps=None,\n",
    "                                  throttle_secs = EVAL_AFTER_SEC)# evalute after each 15 training seconds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-ac63b27d0192>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSERVING_HEADER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'columns' is not defined"
     ]
    }
   ],
   "source": [
    "features = dict(zip(SERVING_HEADER, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
