{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "#tf.enable_eager_execution()\n",
    "print(tf.__version__)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df_dataset = pandas.read_csv('data/train-data.csv',names=HEADER,skiprows=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'reg-model-02'\n",
    "\n",
    "TRAIN_DATA_FILES_PATTERN = 'data/train-*.csv'\n",
    "VALID_DATA_FILES_PATTERN = 'data/valid-*.csv'\n",
    "TEST_DATA_FILES_PATTERN = 'data/test-*.csv'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = True\n",
    "MULTI_THREADING = True\n",
    "\n",
    "HEADER = ['key','x','y','alpha','beta','target']\n",
    "HEADER_DEFAULTS = [[0], [0.0], [0.0], ['NA'], ['NA'], [0.0]]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['x', 'y']  \n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'alpha':['ax01', 'ax02'], 'beta':['bx01', 'bx02']}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TARGET_NAME = 'target'\n",
    "\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['data\\\\train-data.csv'],\n",
       " True,\n",
       " array([b'data\\\\train-data.csv'], dtype=object))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gfile.Glob(TRAIN_DATA_FILES_PATTERN),tf.gfile.Exists('data\\\\train-data.csv'),sess.run(tf.matching_files(TRAIN_DATA_FILES_PATTERN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'E:\\my_proj\\fog_recognition\\ExtendFoGData\\fixed_data\\SignalETL4FoG'\n",
    "filenames = tf.gfile.ListDirectory(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n",
      "[4. 6.]\n",
      "[3. 5.]\n",
      "[11.  2.]\n",
      "[41.  6.]\n",
      "[13.  5.]\n"
     ]
    }
   ],
   "source": [
    "#数据输入转换管道\n",
    "X= tf.placeholder(tf.float32,shape=(None,2))\n",
    "ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# 迭代器部分\n",
    "iterator = ds.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "sess.run(iterator.initializer,feed_dict={X:[[1.0,2],[4,6],[3,5],[7,8],[11,9]]})\n",
    "for i in range(3):\n",
    "    print(sess.run(next_element))\n",
    "    \n",
    "sess.run(iterator.initializer,feed_dict={X:[[11.0,2],[41,6],[13,5],[27,48],[141,59]]})\n",
    "for i in range(3):\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n",
      "[4. 6.]\n",
      "[3. 5.]\n",
      "================================\n",
      "[11.  2.]\n",
      "[41.  6.]\n",
      "[13.  5.]\n",
      "================================\n",
      "[81.  2.]\n",
      "[41. 66.]\n",
      "[13. 75.]\n"
     ]
    }
   ],
   "source": [
    "#数据输入转换管道\n",
    "X= tf.placeholder(tf.float32,shape=(None,2))\n",
    "ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# 迭代器部分\n",
    "iterator = ds.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init = iterator.make_initializer(ds)\n",
    "\n",
    "sess.run(iterator.initializer,feed_dict={X:[[1.0,2],[4,6],[3,5],[7,8],[11,9]]})\n",
    "for i in range(3):\n",
    "    print(sess.run(next_element))\n",
    "print(\"================================\")   \n",
    "sess.run(iterator.initializer,feed_dict={X:[[11.0,2],[41,6],[13,5],[27,48],[141,59]]})\n",
    "for i in range(3):\n",
    "    print(sess.run(next_element))\n",
    "print(\"================================\")\n",
    "sess.run(init,feed_dict={X:[[81.0,2],[41,66],[13,75],[27,8],[41,5]]})\n",
    "for i in range(3):\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure. 客户端的感觉\n",
    "training_dataset = tf.data.Dataset.range(100).map(lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "# A reinitializable iterator is defined by its structure. We could use the\n",
    "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
    "# or `validation_dataset` here, because they are compatible.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "# 使用两个不同数据数据集来初始化迭代器\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "# Run 20 epochs in which the training dataset is traversed, followed by the validation dataset.\n",
    "for _ in range(20):\n",
    "  # Initialize an iterator over the training dataset.\n",
    "    sess.run(training_init_op)\n",
    "    for _ in range(100):\n",
    "        print(sess.run(next_element))\n",
    "  # Initialize an iterator over the validation dataset.\n",
    "    sess.run(validation_init_op)\n",
    "    for _ in range(50):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(lambda x: x + tf.random_uniform([], -10, 10, tf.int64)).repeat()\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "# A feedable iterator is defined by a handle placeholder and its structure. We\n",
    "# could use the `output_types` and `output_shapes` properties of either\n",
    "# `training_dataset` or `validation_dataset` here, because they have identical structure.\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# You can use feedable iterators with a variety of different kinds of iterator (such as one-shot and initializable iterators).\n",
    "training_iterator = training_dataset.make_one_shot_iterator()  # 直接从训练数据集上创建新的迭代器\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "# The `Iterator.string_handle()` method returns a tensor that can be evaluated and used to feed the `handle` placeholder.\n",
    "training_handle = sess.run(training_iterator.string_handle()) # 获得待输入迭代器的handle\n",
    "validation_handle = sess.run(validation_iterator.string_handle())\n",
    "\n",
    "# Loop forever, alternating between training and validation.\n",
    "for _ in range(100):\n",
    "  # Run 200 steps using the training dataset. Note that the training dataset is\n",
    "  # infinite, and we resume from where we left off in the previous `while` loop iteration.\n",
    "    for _ in range(200):\n",
    "        sess.run(next_element, feed_dict={handle: training_handle}) # 用获得数据的handle喂作为handle的placeholder，进一步到next_element\n",
    "\n",
    "  # Run one pass over the validation dataset.\n",
    "    sess.run(validation_iterator.initializer)\n",
    "    for _ in range(50):\n",
    "        sess.run(next_element, feed_dict={handle: validation_handle})\n",
    "        \n",
    "# 总结一下就是并存三个迭代器对象，而不像之前只有一个，通过重新初始化便可多次复用；当前的情况就是，另外两个借助一个迭代器到达next_element;\n",
    "# 想汇流一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_handle.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TextLineDataset(filenames=file_names)\n",
    "dataset = dataset.skip(skip_header_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_csv_row(row):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'data\\\\train-data.csv'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_input_fn():\n",
    "  filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "  dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "  # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "  # protocol buffer, and perform any additional per-record preprocessing.\n",
    "  def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image_data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"date_time\": tf.FixedLenFeature((), tf.int64, default_value=\"\"),\n",
    "        \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                    default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    # Perform additional preprocessing on the parsed data.\n",
    "    image = tf.image.decode_jpeg(parsed[\"image_data\"])\n",
    "    image = tf.reshape(image, [299, 299, 1])\n",
    "    label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "\n",
    "    return {\"image_data\": image, \"date_time\": parsed[\"date_time\"]}, label\n",
    "\n",
    "  # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "  # tensor for each example.\n",
    "  dataset = dataset.map(parser)\n",
    "  dataset = dataset.shuffle(buffer_size=10000)\n",
    "  dataset = dataset.batch(32)\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "  # `features` is a dictionary in which each value is a batch of values for\n",
    "  # that feature; `labels` is a batch of labels.\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset('data\\\\train-data.csv')\n",
    "input_file_names = tf.train.match_filenames_once(pattern=TRAIN_DATA_FILES_PATTERN)\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': <tf.Tensor 'DecodeCSV_39:3' shape=() dtype=string>,\n",
       " 'beta': <tf.Tensor 'DecodeCSV_39:4' shape=() dtype=string>,\n",
       " 'key': <tf.Tensor 'DecodeCSV_39:0' shape=() dtype=int32>,\n",
       " 'target': <tf.Tensor 'DecodeCSV_39:5' shape=() dtype=float32>,\n",
       " 'x': <tf.Tensor 'DecodeCSV_39:1' shape=() dtype=float32>,\n",
       " 'y': <tf.Tensor 'DecodeCSV_39:2' shape=() dtype=float32>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(HEADER, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15196, 0.8112191, -0.35508192, b'ax02', b'bx02', 0.38995337],\n",
       " {'alpha': b'ax02',\n",
       "  'beta': b'bx02',\n",
       "  'key': 17262,\n",
       "  'target': -7.7214413,\n",
       "  'x': 0.24147429,\n",
       "  'y': -1.0556995}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)  # 返回张量本地列表\n",
    "    features = dict(zip(HEADER, columns))  # 张量的字典，而且还是本地字典，Python语言对象，张量的数据结构不够用了吧？\n",
    "    for column in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(column)  #典型Python列表操作\n",
    "    target = features.pop(TARGET_NAME)\n",
    "    return features, target\n",
    "\n",
    "def process_features(features):\n",
    "    # 典型字典键值对操作\n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y']) # features['x'] * features['y']\n",
    "    features['dist_xy'] =  tf.sqrt(tf.squared_difference(features['x'],features['y']))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': <tf.Tensor 'DecodeCSV_39:3' shape=() dtype=string>,\n",
       " 'beta': <tf.Tensor 'DecodeCSV_39:4' shape=() dtype=string>,\n",
       " 'key': <tf.Tensor 'DecodeCSV_39:0' shape=() dtype=int32>,\n",
       " 'target': <tf.Tensor 'DecodeCSV_39:5' shape=() dtype=float32>,\n",
       " 'x': <tf.Tensor 'DecodeCSV_39:1' shape=() dtype=float32>,\n",
       " 'y': <tf.Tensor 'DecodeCSV_39:2' shape=() dtype=float32>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = data.TextLineDataset(filenames=file_names)\n",
    "dataset = dataset.skip(skip_header_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'data\\\\train-data.csv']\n",
      "==============\n",
      "[17262, 0.24147429, -1.0556995, b'ax02', b'bx02', -7.7214413] \n",
      " b'4554,0.10051707170557876,1.2955174458337284,ax01,bx01,-20.770956632825076'\n",
      "[19003, 0.12530836, -0.37078774, b'ax02', b'bx02', -6.0008817] \n",
      " b'15196,0.8112191149808337,-0.3550819305486099,ax02,bx02,0.3899533765565255'\n",
      "[9502, 0.09195055, -0.61742663, b'ax02', b'bx01', -3.9802465] \n",
      " b'19474,-0.11787064759978083,0.6238653118217099,ax02,bx02,5.016727463800235'\n",
      "[3302, -0.4787028, -0.22541492, b'ax01', b'bx01', 3.7519367] \n",
      " b'11529,-0.012568233321424427,0.6128445291658935,ax01,bx02,30.278346073204702'\n",
      "[16197, 0.29961985, -0.457031, b'ax02', b'bx02', -35.705055] \n",
      " b'19710,0.03385831765171865,0.8426745068240074,ax02,bx02,15.545556636113083'\n",
      "[12354, 0.19432136, -0.5721079, b'ax01', b'bx02', -33.327343] \n",
      " b'8122,0.46105974898299296,-0.6101714760168783,ax02,bx01,3.6019083787146724'\n",
      "[640, -0.29774538, -0.2000615, b'ax01', b'bx01', 1.1973482] \n",
      " b'6092,0.06519160224744172,-0.38465389194738503,ax02,bx01,-1.5133615069273394'\n",
      "[9147, -0.22898157, 0.6589973, b'ax02', b'bx01', -4.5228047] \n",
      " b'15927,0.7640501123344947,0.06052474477097047,ax02,bx02,-4.164519740963884'\n",
      "[9853, -0.034321357, -1.0857294, b'ax02', b'bx01', -13.8888] \n",
      " b'5673,0.5729000966967237,-0.6048170234111765,ax02,bx01,9.581450875537628'\n",
      "[11246, -0.72309124, 1.0955362, b'ax01', b'bx02', 8.921983] \n",
      " b'4158,-0.27298679807128373,0.015217038265066193,ax01,bx01,-1.1962326948021447'\n",
      "[5794, 0.5278039, 0.7070823, b'ax02', b'bx01', 18.642746] \n",
      " b'11435,-0.08749173658673515,0.7730844470300999,ax01,bx02,35.45139501075312'\n",
      "[13589, 0.91085017, -0.14354855, b'ax01', b'bx02', -10.915428] \n",
      " b'4023,-0.9881931484967852,-0.30948714363322893,ax01,bx01,2.088923490870729'\n",
      "[17398, -0.30915678, 0.47668487, b'ax02', b'bx02', 18.94283] \n",
      " b'7606,-0.033983487450379185,-0.9060912454860783,ax02,bx01,-9.295346073493205'\n",
      "[18740, -0.44472286, 0.4369965, b'ax02', b'bx02', -3.0670793] \n",
      " b'7461,-0.40373433450917384,-0.519419087628738,ax02,bx01,-6.68274431880897'\n",
      "[8502, 0.22198798, 0.044871684, b'ax02', b'bx01', -2.0299728] \n",
      " b'6058,-0.8729967653385795,-0.7072570730151782,ax02,bx01,-27.02709176463765'\n",
      "[6852, 0.7128743, 0.64977086, b'ax02', b'bx01', 19.215796] \n",
      " b'3588,0.0043583738333231176,-0.1751291995545345,ax01,bx01,1.662499538564305'\n",
      "[8900, -0.060242068, -0.08344919, b'ax02', b'bx01', 1.5513546] \n",
      " b'10914,-0.8537951834004501,0.4105054383596207,ax01,bx02,-3.5486092984808035'\n",
      "[15205, -0.12918535, 0.51899177, b'ax02', b'bx02', -32.399227] \n",
      " b'6986,-0.1495311682164762,-0.72784721528954,ax02,bx01,-8.12566287129412'\n",
      "[13158, 0.5671823, -0.40958902, b'ax01', b'bx02', -28.56626] \n",
      " b'5022,-0.6126504175489385,0.08933467101059722,ax02,bx01,0.4474575369759134'\n",
      "[10398, -1.2327414, 0.14676815, b'ax01', b'bx02', -6.5801125] \n",
      " b'10103,0.2326745269962045,0.5576936542050417,ax01,bx02,33.29020463630927'\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(input_file_names))\n",
    "    #print(sess.run(dataset))\n",
    "    print(\"==============\")\n",
    "    for i in range(20):\n",
    "        columns = tf.decode_csv(next_element, record_defaults=HEADER_DEFAULTS)\n",
    "        print(sess.run(columns),'\\n',sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "for i in range(0,10,2):\n",
    "    print(sess.run(next_element))\n",
    "\n",
    "# Initialize the same iterator over a dataset with 100 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "for i in range(0,40,5):\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(max_value):\n",
    "    dataset = tf.data.Dataset.range(max_value)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = f2(22)\n",
    "next_element = it.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sess.run(it.initializer)\n",
    "for i in range(0,10,2):\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return tf.matmul(x,[[2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(f1([[3.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=f1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "for i in range(0,10,2):\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "max_value =20\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "sess.run(iterator.initializer)\n",
    "for i in range(0,10,2):\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def data_iterator(max_value):\n",
    "    dataset = tf.data.Dataset.range(max_value)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator\n",
    "\n",
    "iterator = data_iterator(10)\n",
    "next_element = iterator.get_next()\n",
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "sess.run(iterator.initializer)\n",
    "for i in range(0,10,2):\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
