{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tensorflow.python.feature_column import feature_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用TF Estimator的基本步骤\n",
    "1. 定义数据的元数据用于解析\n",
    "2. 定义Estimator的输入函数，以完成从dataframe中读取数据并使用特征处理（ETL）\n",
    "3. 基于元数据创建Estimator的特征列和扩展特征列（特征提取变换）\n",
    "4. 根据特征列和超参数创建估计器实例\n",
    "5. 使用数据训练估计器\n",
    "6. 使用测试数据评估估计器\n",
    "7. 使用估计其执行预测\n",
    "8. 保存和部署估计器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME ='reg-model-01'\n",
    "TRAIN_FILE = 'data/train-data.csv'\n",
    "VALID_FILE = 'data/valid-data.csv'\n",
    "TEST_FILE = 'data/test-data.csv'\n",
    "\n",
    "# 可选控制参数，不是通过设置属性实现，整体采用函数式编程\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = True\n",
    "MULTI_THREADING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义数据集的元信息\n",
    "1. CSV文件的头及其默认值\n",
    "2. 数值和类型特征列的名字\n",
    "3. 目标特征的名字\n",
    "4. 无用列名字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义各种输入常量\n",
    "HEADER= ['key','x','y','alpha','beta','target']\n",
    "\n",
    "HEADER_DEFAULTS = [[0], [0.0], [0.0], ['NA'], ['NA'], [0.0]]  # 像spark一样设置一些data的元数据，如读取类型等,sparkRDD schema\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['x', 'y']  \n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'alpha':['ax01', 'ax02'], 'beta':['bx01', 'bx02']} # 指定类别特征及其取值范围\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TARGET_NAME = 'target'\n",
    "\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME}) #无用列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.定义输入函数\n",
    "1. 输入文件的名字\n",
    "2. 加载pandas DataFrame\n",
    "3. 使用处理函数\n",
    "4. 返回的函数，这个函数能够返回（特征，目标）张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(dataset_df):\n",
    "    \"\"\"这里用的是numpy的square而不是tf.square,有点不同\"\"\"\n",
    "    dataset_df['x_2']=np.square(dataset_df['x']) \n",
    "    dataset_df[\"y_2\"] = np.square(dataset_df['y'])\n",
    "    dataset_df[\"xy\"] = dataset_df['x'] * dataset_df['y']\n",
    "    dataset_df['dist_xy'] =  np.sqrt(np.square(dataset_df['x']-dataset_df['y']))\n",
    "    return dataset_df\n",
    "\n",
    "def generate_pandas_input_fn(file_name,mode=tf.estimator.ModeKeys.EVAL,\n",
    "                             skip_header_line=0,num_epochs=1,batch_size=100):\n",
    "    \"\"\"直接使用全局常量，而不是采用传递的方式或成员属性，常量的作用域在此模块内；\n",
    "       函数签名参数采用传递方式进入，因为其经常变化待输入\n",
    "    \"\"\"\n",
    "    df_dataset=pd.read_csv(file_name,names=HEADER)\n",
    "    x=df_dataset[FEATURE_NAMES].copy()\n",
    "    if PROCESS_FEATURES:\n",
    "        x=process_dataframe(x)\n",
    "    y=df_dataset[TARGET_NAME]\n",
    "    shuffle=True if mode==tf.estimator.ModeKeys.TRAIN else False\n",
    "    num_threads=1\n",
    "    if MULTI_THREADING:\n",
    "        num_threads=multiprocessing.cpu_count()\n",
    "        num_epochs =  num_epochs = int(num_epochs/num_threads) if mode == tf.estimator.ModeKeys.TRAIN else num_epochs\n",
    "    # 以上都是在制备pandas_input_fn的参数值\n",
    "    pandas_input_fn=tf.estimator.inputs.pandas_input_fn(x=x,y=y,batch_size=batch_size,num_epochs= num_epochs,\n",
    "                                                        shuffle=shuffle,target_column=TARGET_NAME)\n",
    "    return pandas_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature read from DataFrame: ['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']\n",
      "Target read from DataFrame: Tensor(\"fifo_queue_DequeueUpTo_1:9\", shape=(?,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "features, target = generate_pandas_input_fn(file_name=TRAIN_FILE)()  #调用generate_pandas_input_fn返回的函数\n",
    "print(\"Feature read from DataFrame: {}\".format(list(features.keys())))\n",
    "print(\"Target read from DataFrame: {}\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义特征列\n",
    "先假设数值输被正规化或者具有相同的尺度，否则，正规化函数及其参数应当被输入到数值特征列构造器中去."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'x': _NumericColumn(key='x', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y': _NumericColumn(key='y', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'x_2': _NumericColumn(key='x_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y_2': _NumericColumn(key='y_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'xy': _NumericColumn(key='xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'dist_xy': _NumericColumn(key='dist_xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'alpha': _VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'beta': _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'alpha_X_beta': _CrossedColumn(keys=(_VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=4, hash_key=None)}\n"
     ]
    }
   ],
   "source": [
    "def get_feature_columns():   \n",
    "    \"\"\"将各种特征的制备定义在函数内部，方便后面主要调用流程的组装\"\"\"\n",
    "    all_numeric_feature_names = NUMERIC_FEATURE_NAMES\n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = ['x_2', 'y_2', 'xy', 'dist_xy']\n",
    "    if PROCESS_FEATURES:\n",
    "        all_numeric_feature_names += CONSTRUCTED_NUMERIC_FEATURES_NAMES\n",
    "\n",
    "    numeric_columns = {feature_name: tf.feature_column.numeric_column(feature_name) for feature_name in all_numeric_feature_names}\n",
    "    # 根据特征构造特征名称及其对象的字典\n",
    "    categorical_column_with_vocabulary = \\\n",
    "        {item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "        \n",
    "    feature_columns = {}\n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "    # add extended features (crossing, bucektization, embedding)\n",
    "    feature_columns['alpha_X_beta'] = tf.feature_column.crossed_column([feature_columns['alpha'], feature_columns['beta']], 4)\n",
    "    return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.创建估计器\n",
    "1. 定义一个估计器创建函数\n",
    "2. 设置超参数及其运行参数\n",
    "3. 初始化一个估计器实例  ----前面都是在定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(run_config,hparams):\n",
    "    feature_columns = list(get_feature_columns().values())  #产生特征列\n",
    "    dense_columns = list(filter(lambda column: isinstance(column, feature_column._NumericColumn),feature_columns))\n",
    "    # 筛查出验证各种特征\n",
    "    categorical_columns = list(filter(lambda column: isinstance(column, feature_column._VocabularyListCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._BucketizedColumn),feature_columns))\n",
    "    indicator_columns = list(map(lambda column: tf.feature_column.indicator_column(column),categorical_columns))    \n",
    "    estimator_feature_columns = dense_columns + indicator_columns \n",
    "    # 准备好了各种特征列和参数制备估计器\n",
    "    estimator = tf.estimator.DNNRegressor(feature_columns= estimator_feature_columns,hidden_units= hparams.hidden_units, \n",
    "                                          optimizer= tf.train.AdamOptimizer(),activation_fn= tf.nn.elu,\n",
    "                                          dropout= hparams.dropout_prob,config= run_config)\n",
    "    print(\"Estimator Type: {}\".format(type(estimator)))\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这与传统及其学习估计器不同，其值定义好并并指导网络结构的生成，最后固定下来形成一个对象。\n",
    "hparams  = tf.contrib.training.HParams(num_epochs = 100,batch_size = 500,hidden_units=[8, 4],dropout_prob = 0.0)\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "run_config = tf.estimator.RunConfig().replace(model_dir=model_dir)\n",
    "print(\"Model directory: {}\".format(run_config.model_dir))\n",
    "print(\"Hyper-parameters: {}\".format(hparams))\n",
    "\n",
    "estimator = create_estimator(run_config, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########定义和声明过程结束，下面部分进行脚本调用################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.训练估计器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练估计器需要使用到输入数据\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "train_input_fn=generate_pandas_input_fn(file_name= TRAIN_FILE, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                         num_epochs=hparams.num_epochs,batch_size=hparams.batch_size) \n",
    "if not RESUME_TRAINING:\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"Estimator training started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\")\n",
    "\n",
    "# 通常简单训练一下\n",
    "estimator.train(input_fn = train_input_fn)\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Estimator training finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Estimator training elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-22-06:45:50\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01\\model.ckpt-2400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-22-06:45:51\n",
      "INFO:tensorflow:Saving dict for global step 2400: average_loss = 121.92495, global_step = 2400, label/mean = 1.108437, loss = 609624.75, prediction/mean = 1.7788389\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2400: trained_models/reg-model-01\\model.ckpt-2400\n",
      "\n",
      "{'average_loss': 121.92495, 'label/mean': 1.108437, 'loss': 609624.75, 'prediction/mean': 1.7788389, 'global_step': 2400}\n",
      "\n",
      "RMSE: 11.04196\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "TEST_SIZE = 5000\n",
    "test_input_fn = generate_pandas_input_fn(file_name=TEST_FILE,mode= tf.estimator.ModeKeys.EVAL,batch_size= TEST_SIZE)\n",
    "\n",
    "results = estimator.evaluate(input_fn=test_input_fn)\n",
    "print(\"\")\n",
    "print(results)\n",
    "rmse = round(math.sqrt(results[\"average_loss\"]),5)\n",
    "print(\"\")\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01\\model.ckpt-2400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Predicted Values: [32.544582, 7.4533186, -0.5131136, 0.002710577, 3.5384486]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "predict_input_fn = generate_pandas_input_fn(file_name=TEST_FILE,mode= tf.estimator.ModeKeys.PREDICT, batch_size= 5)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn)\n",
    "values = list(map(lambda item: item[\"predictions\"][0],list(itertools.islice(predictions, 5))))\n",
    "print()\n",
    "print(\"Predicted Values: {}\".format(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 保存 & 部署  Model\n",
    "进入模型部署阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.定义服务函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(features):\n",
    "    \"\"\"采用tf.square等函数处理输入的特征\"\"\"\n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y'])\n",
    "    features['dist_xy'] =  tf.sqrt(tf.squared_difference(features['x'],features['y']))\n",
    "    return features\n",
    "\n",
    "def csv_serving_input_fn():\n",
    "    \"\"\"服务输入函数与训练的不同\"\"\"\n",
    "    SERVING_HEADER = ['x','y','alpha','beta']\n",
    "    SERVING_HEADER_DEFAULTS = [[0.0], [0.0], ['NA'], ['NA']]\n",
    "    \n",
    "    # 数据来源是一个占位符\n",
    "    rows_string_tensor = tf.placeholder(dtype=tf.string, shape=[None], name='csv_rows')\n",
    "    \n",
    "    receiver_tensor = {'csv_rows': rows_string_tensor} \n",
    "\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "    # 将接受到的数据解析成特征字段的子弹\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=SERVING_HEADER_DEFAULTS)\n",
    "    features = dict(zip(SERVING_HEADER, columns))\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        features = process_features(features)\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 结合服务函数导出模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = model_dir + \"/export\"\n",
    "\n",
    "estimator.export_savedmodel(export_dir_base = export_dir,serving_input_receiver_fn = csv_serving_input_fn, as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['serving_default', 'regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01\\model.ckpt-2400\n",
      "WARNING:tensorflow:From D:\\ProgramLanguageCore\\Python\\anaconda351\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/reg-model-01/export\\temp-b'1542869564'\\saved_model.pbtxt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'trained_models/reg-model-01/export\\\\1542869564'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.Exporter.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.部署保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/reg-model-01/export/1542869564\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/export/1542869564\\variables\\variables\n",
      "{'predictions': array([[ 55.15245],\n",
      "       [-14.56848]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1] \n",
    "\n",
    "print(saved_model_dir)\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(export_dir = saved_model_dir, signature_def_key=\"predict\")\n",
    "\n",
    "output = predictor_fn({'csv_rows': [\"0.5,1,ax01,bx02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained_models/reg-model-01/export/1542869564'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.pre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
